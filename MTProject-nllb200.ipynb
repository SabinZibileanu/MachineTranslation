{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:14:44.512058Z",
     "start_time": "2025-02-04T00:14:42.710772Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Loading the data\n",
    "def load_parallel_data(romanian_path, romani_path):\n",
    "    with open(romanian_path, 'r', encoding='utf-8') as ro_file, \\\n",
    "         open(romani_path, 'r', encoding='utf-8') as roma_file:\n",
    "        romanian_lines = ro_file.readlines()\n",
    "        romani_lines = roma_file.readlines()\n",
    "\n",
    "    assert len(romanian_lines) == len(romani_lines), \"Mismatched number of lines!\"\n",
    "\n",
    "    data = {\"translation\": [{\"ro\": ro.strip(), \"roma\": roma.strip()}\n",
    "                              for ro, roma in zip(romanian_lines, romani_lines)]}\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "huggingface_dataset = load_parallel_data('/kaggle/input/sptop-dataset/romanian.txt', '/kaggle/input/sptop-dataset/romani.txt')\n",
    "huggingface_dataset_shuffled = huggingface_dataset.shuffle(seed = 42)\n",
    "train_test_split = huggingface_dataset_shuffled.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_eval_split = train_test_split[\"test\"].train_test_split(test_size = 0.5)\n",
    "\n",
    "eval_dataset = test_eval_split['train']\n",
    "test_dataset = test_eval_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:14:44.614018Z",
     "start_time": "2025-02-04T00:14:44.515245Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset['translation'])\n",
    "val_df = pd.DataFrame(eval_dataset['translation'])\n",
    "test_df = pd.DataFrame(test_dataset['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:14:45.069331Z",
     "start_time": "2025-02-04T00:14:44.689606Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \"):\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    # replace ùìïùîØùîûùî´ùî†ùî¢ùî∞ùî†ùîû by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def translate_and_evaluate(input_sentences, correct_translations, \n",
    "                          src_lang, tgt_lang, a=32, b=3, \n",
    "                          max_input_length=1024, num_beams=4, batch_size=16):\n",
    "    # Ensure we have same number of inputs and references\n",
    "    assert len(input_sentences) == len(correct_translations), \\\n",
    "        \"Input sentences and correct translations must be same length\"\n",
    "    \n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = int(np.ceil(len(input_sentences) / batch_size))\n",
    "    \n",
    "    # Translate all sentences\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Translating batches\"):\n",
    "        # Get batch start/end indices\n",
    "        start = batch_idx * batch_size\n",
    "        end = min(start + batch_size, len(input_sentences))\n",
    "        batch_inputs = input_sentences[start:end]\n",
    "        \n",
    "        # Tokenize input with source language code\n",
    "        tokenizer.src_lang = src_lang\n",
    "        tokenizer.tgt_lang = tgt_lang\n",
    "        inputs = tokenizer(batch_inputs.tolist(), return_tensors=\"pt\", padding=True, truncation=True, \n",
    "                          max_length=max_input_length).to(model.device)\n",
    "        \n",
    "        # Generate translation\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs.to(model.device),\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "            max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "            num_beams=num_beams,\n",
    "        )\n",
    "        \n",
    "        # Decode generated tokens\n",
    "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    # Tokenize references and hypotheses\n",
    "    references = [[nltk.word_tokenize(ref)] for ref in correct_translations]\n",
    "    hypotheses = [nltk.word_tokenize(hyp) for hyp in translations]\n",
    "    \n",
    "    # Use smoothing function to avoid zero scores\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothing)\n",
    "    \n",
    "    return translations, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:14:48.268279Z",
     "start_time": "2025-02-04T00:14:45.077580Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:14:48.755581Z",
     "start_time": "2025-02-04T00:14:48.337669Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "model.cuda()\n",
    "optimizer = Adafactor(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:14:48.796460Z",
     "start_time": "2025-02-04T00:14:48.794506Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Try to free GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:15:22.314039Z",
     "start_time": "2025-02-04T00:15:22.311310Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_length = 128\n",
    "num_epochs = 30\n",
    "avg_losses = []\n",
    "bleu_scores = []\n",
    "MODEL_SAVE_PATH = '/kaggle/working/nllb-ro-roma-v2'\n",
    "\n",
    "LANGS = [('ro', 'ro_Latn'), ('roma', 'roma_Latn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T00:23:21.861213Z",
     "start_time": "2025-02-04T00:15:24.073352Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "best_bleu_score = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle dataset indices at start of each epoch\n",
    "    data_len = len(train_df)\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "    num_batches = (data_len + batch_size - 1) // batch_size  # Include partial batch\n",
    "    \n",
    "    epoch_losses = []\n",
    "    cleanup()\n",
    "    \n",
    "    # Epoch progress bar with batch-level updates\n",
    "    tq = trange(num_batches, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_idx in tq:\n",
    "        # Randomly select language pair for this batch\n",
    "        (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n",
    "        \n",
    "        # Get batch data from shuffled indices\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_indices = shuffled_indices[start:end]\n",
    "        \n",
    "        # Prepare batch data\n",
    "        xx, yy = [], []\n",
    "        for idx in batch_indices:\n",
    "            item = train_df.iloc[idx]\n",
    "            xx.append(preproc(item[l1]))\n",
    "            yy.append(preproc(item[l2]))\n",
    "        \n",
    "        try:\n",
    "            # Tokenization and model processing\n",
    "            tokenizer.src_lang = long1\n",
    "            x = tokenizer(xx, return_tensors='pt', padding=True, \n",
    "                         truncation=True, max_length=max_length).to(model.device)\n",
    "            \n",
    "            tokenizer.src_lang = long2\n",
    "            y = tokenizer(yy, return_tensors='pt', padding=True,\n",
    "                         truncation=True, max_length=max_length).to(model.device)\n",
    "            y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            # Forward pass and training\n",
    "            loss = model(**x, labels=y.input_ids).loss\n",
    "            loss.backward()\n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar with current batch loss\n",
    "            tq.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            cleanup()\n",
    "            print(f'Error in epoch {epoch+1}, batch {batch_idx}: {str(e)}')\n",
    "            continue\n",
    "\n",
    "    # After completing all batches in epoch\n",
    "    avg_epoch_loss = np.mean(epoch_losses)\n",
    "    avg_losses.append(avg_epoch_loss)\n",
    "    print(f'Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "    # Compute validation BLEU score\n",
    "    _, bleu_score = translate_and_evaluate(\n",
    "        val_df['ro'], val_df['roma'], 'ro_Latn', 'roma_Latn'\n",
    "    )\n",
    "    bleu_scores.append(bleu_score)\n",
    "    print(f'BLEU score: {bleu_score:.4f}')\n",
    "    \n",
    "    if bleu_score > best_bleu_score:\n",
    "        # Save model checkpoint\n",
    "        model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "        print(\"Saving model...\")\n",
    "\n",
    "        best_bleu_score = bleu_score\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/losses.txt\", \"w\") as f:\n",
    "    for loss in avg_losses:\n",
    "        f.write(f\"{loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/bleu_scores.txt\", \"w\") as f:\n",
    "    for score in bleu_scores:\n",
    "        f.write(f\"{score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_bleu = translate_and_evaluate(\n",
    "    test_df['ro'], test_df['roma'], 'ro_Latn', 'roma_Latn'\n",
    ")\n",
    "with open(\"/kaggle/working/test_bleu.txt\", \"w\") as f:\n",
    "    f.write(f\"{test_bleu:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6598348,
     "sourceId": 10655417,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
