{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "IC_5DssHzYuR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "import sentencepiece as spm\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import MarianTokenizer, MarianMTModel, MarianConfig\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_yOzudXamtm"
      },
      "source": [
        "Scraping the data from different websites/dictionaries/romani courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "hjDBYQ-cOlL1"
      },
      "outputs": [],
      "source": [
        "class Scraper:\n",
        "  # This scraper will help us gather romani texts from certain websites\n",
        "\n",
        "  def __init__(self):\n",
        "    self.romani_data_output_path = 'romani.txt'\n",
        "    self.romanian_data_output_path = 'romanian.txt'\n",
        "\n",
        "  def wikipedia_scraper(self, url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    forbidden_pattern_list = [r'<a\\s+href=\"/wiki/[^\"]+:[^\"]+\"', r'<a\\s+href=\"http[^\"]*\"', r'<a\\s+href=\"\\/w\\/index\\.php\\?[^\"]*\"[^>]*>.*?<\\/a>']\n",
        "    references = soup.find_all('a', href = True, attrs = {'class':'', 'span':'', 'accesskey':'', 'data-mw':'', 'dir':'', 'aria-label':''})\n",
        "    romani_phrases_list = [] # This list will help us with checking the duplicates, as this method can store multiple instances of the same phrase\n",
        "\n",
        "    file_writer = open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for ref in references:\n",
        "      if not any(re.search(pattern, str(ref)) for pattern in forbidden_pattern_list):\n",
        "        if ref.get_text() not in romani_phrases_list:\n",
        "          romani_phrases_list.append(ref.get_text())\n",
        "          file_writer.write(ref.get_text()), file_writer.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "  def tumblr_scraper(self, url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    phrases = soup.find_all('p')\n",
        "    auxiliary_phrase_list = [phrase.get_text() for phrase in phrases] # This list helps us store all the phrases from the website\n",
        "\n",
        "    file_writer = open(self.romani_data_output_path, 'a')\n",
        "    \n",
        "    # Now we will only select the romani phrases\n",
        "    for phrase in auxiliary_phrase_list[1:len(auxiliary_phrase_list) - 1]:\n",
        "      romani_phrase = phrase.split('\\n')\n",
        "      file_writer.write(romani_phrase[0]), file_writer.write('\\n')\n",
        "\n",
        "  def glosbe_romani_scraper(self, url):\n",
        "    # This site helps with different dialects of romani, and so we used balkan and carpathian as it provides phrases as examples\n",
        "    # Note: Some samples were added manually due to some problems with scraping\n",
        "\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    romanian_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pr-1'})\n",
        "    carpathian_romani_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pl-1'})\n",
        "\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for romanian_phrase, romani_phrase in zip(romanian_phrases, carpathian_romani_phrases):\n",
        "      ro_phrase_text, roma_phrase_text = romanian_phrase.get_text().strip(), romani_phrase.get_text().strip()\n",
        "\n",
        "      if ro_phrase_text and roma_phrase_text:\n",
        "        file_writer_romani.write(roma_phrase_text), file_writer_romani.write('\\n')\n",
        "        file_writer_romanian.write(ro_phrase_text), file_writer_romanian.write('\\n')\n",
        "\n",
        "  def get_phrases_from_dictionary_course(self, path_dict):\n",
        "    # This method gets some samples from a romanian - kalderash romani dictionary and a romani course that we found online\n",
        "    \n",
        "    dict_file = open(path_dict, 'r')\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "    for sample in dict_file:\n",
        "      phrases = sample.split(':')\n",
        "      romani_phrase, romanian_phrase = phrases[0], phrases[1].strip()\n",
        "      \n",
        "      file_writer_romani.write(romani_phrase), file_writer_romani.write('\\n')\n",
        "      file_writer_romanian.write(romanian_phrase), file_writer_romanian.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part deals with data augmentation: a first augmentation technique was back translation. The Romanian sentences were translated to English and then the English equivalent was translated back into Romanian, while the Romani samples were kept the same.\n",
        "The second augmentation technique was sentence insertion (beginning and end). Precisely: we introduced a token at the begining and at the end of each romanian and romani sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation (removed random swap due to the semantic ambiguity that it generates)\n",
        "\n",
        "class DataAugmenter:\n",
        "  def __init__(self):\n",
        "    self.romani_data_path = 'romani.txt'\n",
        "    self.romanian_data_path = 'romanian.txt'\n",
        "  \n",
        "  def back_translation(self):\n",
        "    with open(self.romanian_data_path, 'r') as romanian_samples_file, open(self.romani_data_path, 'r') as romani_samples_file:\n",
        "      romanian_samples = romanian_samples_file.readlines()\n",
        "      romani_samples = romani_samples_file.readlines()\n",
        "    \n",
        "    romanian_file_writer, romani_file_writer = open(self.romanian_data_path, 'a'), open(self.romani_data_path, 'a')\n",
        "    romanian_file_writer.write('\\n'), romani_file_writer.write('\\n')\n",
        "\n",
        "    for sample_ro, sample_romani in zip(romanian_samples, romani_samples):\n",
        "      english_equivalent = GoogleTranslator(source='ro', target='en').translate(sample_ro)\n",
        "      back_translated = GoogleTranslator(source='en', target='ro').translate(english_equivalent)\n",
        "      romanian_file_writer.write(back_translated), romanian_file_writer.write('\\n')\n",
        "      romani_file_writer.write(sample_romani.strip()), romani_file_writer.write('\\n')\n",
        "  \n",
        "  def sentence_insertion(self):\n",
        "    begining_token_romani = 'Po del chavo - '\n",
        "    begining_token_romanian = 'Începutul propoziției - '\n",
        "\n",
        "    with open(self.romanian_data_path, 'r') as romanian_samples_file, open(self.romani_data_path, 'r') as romani_samples_file:\n",
        "      romanian_samples = romanian_samples_file.readlines()\n",
        "      romani_samples = romani_samples_file.readlines()\n",
        "    \n",
        "    romanian_file_writer, romani_file_writer = open(self.romanian_data_path, 'a'), open(self.romani_data_path, 'a')\n",
        "    romanian_file_writer.write('\\n'), romani_file_writer.write('\\n')\n",
        "\n",
        "    for sample_ro, sample_romani in zip(romanian_samples, romani_samples):\n",
        "      new_romanian_sentence = begining_token_romanian + sample_ro\n",
        "      new_romani_sentence = begining_token_romani + sample_romani\n",
        "\n",
        "      romanian_file_writer.write(new_romanian_sentence.strip()), romanian_file_writer.write('\\n')\n",
        "      romani_file_writer.write(new_romani_sentence.strip()), romani_file_writer.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next step was to create a tokenizer. We decided to use a MarianTokenizer from HelsinkiNLP as it has support for romance languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a tokenizer and a vocabulary\n",
        "def create_tokenizer(corpus_path, tokenizer_model_prefix):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input = corpus_path,\n",
        "      model_prefix = tokenizer_model_prefix,\n",
        "      vocab_size = 2500, # increase size if you augment the data\n",
        "      pad_id=0,\n",
        "      unk_id=1,\n",
        "      bos_id=2,\n",
        "      eos_id=3\n",
        "  )\n",
        "\n",
        "def create_vocabulary(tok_prefix_source, tok_prefix_target):\n",
        "  sp_processor_source = spm.SentencePieceProcessor()\n",
        "  sp_processor_source.load(tok_prefix_source + '.model')\n",
        "\n",
        "  sp_processor_target = spm.SentencePieceProcessor()\n",
        "  sp_processor_target.load(tok_prefix_target + '.model')\n",
        "\n",
        "  vocab_source, vocab_target = {}, {}\n",
        "\n",
        "  for id in range(sp_processor_source.get_piece_size()):\n",
        "    token_source = sp_processor_source.id_to_piece(id)\n",
        "    vocab_source[token_source] = id\n",
        "\n",
        "    token_target = sp_processor_target.id_to_piece(id)\n",
        "    vocab_target[token_target] = id\n",
        "  \n",
        "  return vocab_source, vocab_target\n",
        "\n",
        "def combine_vocabularies(vocab_source, vocab_target):\n",
        "  combined_vocab = {}\n",
        "  output_file = 'vocab.json'\n",
        "\n",
        "  for idx, (token, _) in enumerate(vocab_target.items()):\n",
        "    combined_vocab[token] = int(idx)\n",
        "  \n",
        "  idx_start_source_vocab, counter_non_duplicates = len(combined_vocab), 0 # keeping count of non duplicate items in the dictionary and also maintaining their index\n",
        "\n",
        "  for idx, (token, _) in enumerate(vocab_source.items()):\n",
        "    if token not in combined_vocab:\n",
        "      combined_vocab[token] = idx_start_source_vocab + counter_non_duplicates\n",
        "      \n",
        "      counter_non_duplicates += 1\n",
        "\n",
        "  with open(output_file, 'w') as vocab_file:\n",
        "    json.dump(combined_vocab, vocab_file, indent=4)\n",
        "  \n",
        "create_tokenizer('romani.txt', 'target')\n",
        "create_tokenizer('romanian.txt', 'source')\n",
        "\n",
        "vocab_source, vocab_target = create_vocabulary('source', 'target')\n",
        "combine_vocabularies(vocab_source, vocab_target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtaining the train, test sets and tokenizing them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_PRETRAINED = False\n",
        "\n",
        "# Loading the data\n",
        "def load_parallel_data(romanian_path, romani_path):\n",
        "    with open(romanian_path, 'r', encoding='utf-8') as ro_file, \\\n",
        "         open(romani_path, 'r', encoding='utf-8') as roma_file:\n",
        "        romanian_lines = ro_file.readlines()\n",
        "        romani_lines = roma_file.readlines()\n",
        "\n",
        "    assert len(romanian_lines) == len(romani_lines), \"Mismatched number of lines!\"\n",
        "\n",
        "    data = {\"translation\": [{\"ro\": ro.strip(), \"roma\": roma.strip()}\n",
        "                              for ro, roma in zip(romanian_lines, romani_lines)]}\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "huggingface_dataset = load_parallel_data('romanian.txt', 'romani.txt')\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained('romanian_romani_tokenizer')\n",
        "\n",
        "# Tokenizing the whole dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [example[\"ro\"] for example in examples[\"translation\"]]\n",
        "    targets = [example[\"roma\"] for example in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target = targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "train_test_split = huggingface_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_PRETRAINED:\n",
        "  model_to_train = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-roa-en')\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_to_train) \n",
        "\n",
        "else:\n",
        "  trained_model_config = MarianConfig()\n",
        "  model_to_train = MarianMTModel(config = trained_model_config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
