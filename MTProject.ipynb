{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "IC_5DssHzYuR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "import sentencepiece as spm\n",
        "import json\n",
        "from transformers import MarianTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_yOzudXamtm"
      },
      "source": [
        "Scraping the data from different websites/dictionaries/romani courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "hjDBYQ-cOlL1"
      },
      "outputs": [],
      "source": [
        "class Scraper:\n",
        "  # This scraper will help us gather romani texts from certain websites\n",
        "\n",
        "  def __init__(self):\n",
        "    self.romani_data_output_path = 'romani.txt'\n",
        "    self.romanian_data_output_path = 'romanian.txt'\n",
        "\n",
        "  def wikipedia_scraper(self, url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    forbidden_pattern_list = [r'<a\\s+href=\"/wiki/[^\"]+:[^\"]+\"', r'<a\\s+href=\"http[^\"]*\"', r'<a\\s+href=\"\\/w\\/index\\.php\\?[^\"]*\"[^>]*>.*?<\\/a>']\n",
        "    references = soup.find_all('a', href = True, attrs = {'class':'', 'span':'', 'accesskey':'', 'data-mw':'', 'dir':'', 'aria-label':''})\n",
        "    romani_phrases_list = [] # This list will help us with checking the duplicates, as this method can store multiple instances of the same phrase\n",
        "\n",
        "    file_writer = open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for ref in references:\n",
        "      if not any(re.search(pattern, str(ref)) for pattern in forbidden_pattern_list):\n",
        "        if ref.get_text() not in romani_phrases_list:\n",
        "          romani_phrases_list.append(ref.get_text())\n",
        "          file_writer.write(ref.get_text()), file_writer.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "  def tumblr_scraper(self, url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    phrases = soup.find_all('p')\n",
        "    auxiliary_phrase_list = [phrase.get_text() for phrase in phrases] # This list helps us store all the phrases from the website\n",
        "\n",
        "    file_writer = open(self.romani_data_output_path, 'a')\n",
        "    \n",
        "    # Now we will only select the romani phrases\n",
        "    for phrase in auxiliary_phrase_list[1:len(auxiliary_phrase_list) - 1]:\n",
        "      romani_phrase = phrase.split('\\n')\n",
        "      file_writer.write(romani_phrase[0]), file_writer.write('\\n')\n",
        "\n",
        "  def glosbe_romani_scraper(self, url):\n",
        "    # This site helps with different dialects of romani, and so we used balkan and carpathian as it provides phrases as examples\n",
        "    # Note: Some samples were added manually due to some problems with scraping\n",
        "\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    romanian_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pr-1'})\n",
        "    carpathian_romani_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pl-1'})\n",
        "\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for romanian_phrase, romani_phrase in zip(romanian_phrases, carpathian_romani_phrases):\n",
        "      ro_phrase_text, roma_phrase_text = romanian_phrase.get_text().strip(), romani_phrase.get_text().strip()\n",
        "\n",
        "      if ro_phrase_text and roma_phrase_text:\n",
        "        file_writer_romani.write(roma_phrase_text), file_writer_romani.write('\\n')\n",
        "        file_writer_romanian.write(ro_phrase_text), file_writer_romanian.write('\\n')\n",
        "\n",
        "  def get_phrases_from_dictionary_course(self, path_dict):\n",
        "    # This method gets some samples from a romanian - kalderash romani dictionary and a romani course that we found online\n",
        "    \n",
        "    dict_file = open(path_dict, 'r')\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "    for sample in dict_file:\n",
        "      phrases = sample.split(':')\n",
        "      romani_phrase, romanian_phrase = phrases[0], phrases[1].strip()\n",
        "      \n",
        "      file_writer_romani.write(romani_phrase), file_writer_romani.write('\\n')\n",
        "      file_writer_romanian.write(romanian_phrase), file_writer_romanian.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part deals with data augmentation: a first augmentation technique was back translation. The Romanian sentences were translated to English and then the English equivalent was translated back into Romanian, while the Romani samples were kept the same.\n",
        "The second augmentation technique was sentence insertion (beginning and end). Precisely: we introduced a token at the begining and at the end of each romanian and romani sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation (removed random swap due to the semantic ambiguity that it generates)\n",
        "\n",
        "class DataAugmenter:\n",
        "  def __init__(self):\n",
        "    self.romani_data_path = 'romani.txt'\n",
        "    self.romanian_data_path = 'romanian.txt'\n",
        "  \n",
        "  def back_translation(self):\n",
        "    with open(self.romanian_data_path, 'r') as romanian_samples_file, open(self.romani_data_path, 'r') as romani_samples_file:\n",
        "      romanian_samples = romanian_samples_file.readlines()\n",
        "      romani_samples = romani_samples_file.readlines()\n",
        "    \n",
        "    romanian_file_writer, romani_file_writer = open(self.romanian_data_path, 'a'), open(self.romani_data_path, 'a')\n",
        "    romanian_file_writer.write('\\n'), romani_file_writer.write('\\n')\n",
        "\n",
        "    for sample_ro, sample_romani in zip(romanian_samples, romani_samples):\n",
        "      english_equivalent = GoogleTranslator(source='ro', target='en').translate(sample_ro)\n",
        "      back_translated = GoogleTranslator(source='en', target='ro').translate(english_equivalent)\n",
        "      romanian_file_writer.write(back_translated), romanian_file_writer.write('\\n')\n",
        "      romani_file_writer.write(sample_romani.strip()), romani_file_writer.write('\\n')\n",
        "  \n",
        "  def sentence_insertion(self):\n",
        "    begining_token_romani = 'Po del chavo - '\n",
        "    begining_token_romanian = 'Începutul propoziției - '\n",
        "\n",
        "    with open(self.romanian_data_path, 'r') as romanian_samples_file, open(self.romani_data_path, 'r') as romani_samples_file:\n",
        "      romanian_samples = romanian_samples_file.readlines()\n",
        "      romani_samples = romani_samples_file.readlines()\n",
        "    \n",
        "    romanian_file_writer, romani_file_writer = open(self.romanian_data_path, 'a'), open(self.romani_data_path, 'a')\n",
        "    romanian_file_writer.write('\\n'), romani_file_writer.write('\\n')\n",
        "\n",
        "    for sample_ro, sample_romani in zip(romanian_samples, romani_samples):\n",
        "      new_romanian_sentence = begining_token_romanian + sample_ro\n",
        "      new_romani_sentence = begining_token_romani + sample_romani\n",
        "\n",
        "      romanian_file_writer.write(new_romanian_sentence.strip()), romanian_file_writer.write('\\n')\n",
        "      romani_file_writer.write(new_romani_sentence.strip()), romani_file_writer.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next step was to create a tokenizer. We decided to use a MarianTokenizer from HelsinkiNLP as it has support for romance languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_tokenizer(corpus_path, tokenizer_model_prefix):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input = corpus_path,\n",
        "      model_prefix = tokenizer_model_prefix,\n",
        "      vocab_size = 2500,\n",
        "      pad_id=0,                \n",
        "      unk_id=1,\n",
        "      bos_id=2,\n",
        "      eos_id=3\n",
        "  )\n",
        "\n",
        "create_tokenizer('romanian.txt', 'source')\n",
        "create_tokenizer('romani.txt', 'target')\n",
        "\n",
        "def create_vocabulary_file(target_lang_vocab_file):\n",
        "  target_file = open(target_lang_vocab_file, 'r')\n",
        "  dictionary_target_tokens = {}\n",
        "  counter = 0\n",
        "\n",
        "  for token in target_file:\n",
        "    token_to_write = token.split()[0]\n",
        "    dictionary_target_tokens[token_to_write] = counter\n",
        "    counter += 1\n",
        "  \n",
        "  json_object = json.dumps(dictionary_target_tokens, indent = 4)\n",
        "  \n",
        "  with open(\"vocab.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "create_vocabulary_file('target.vocab')\n",
        "# Todo next: train a model and see if the tokenizer is ok or needs to be improved"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
