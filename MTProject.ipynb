{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "IC_5DssHzYuR"
      },
      "outputs": [],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "from transformers import MarianTokenizer, MarianMTModel, MarianConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "import nlpaug.augmenter.char as nac\n",
        "import numpy as np\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choosing the model and tokenizer method "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_PRETRAINED = True\n",
        "USE_AUGMENTATION = True\n",
        "\n",
        "if USE_PRETRAINED:\n",
        "  model_to_train = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-roa-en')\n",
        "  tokenizer = MarianTokenizer.from_pretrained('romanian_romani_tokenizer_from_pretrained')\n",
        "\n",
        "else:\n",
        "  tokenizer = MarianTokenizer.from_pretrained('romanian_romani_tokenizer')\n",
        "  trained_model_config = MarianConfig()\n",
        "  model_to_train = MarianMTModel(config = trained_model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtaining train, test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "def load_parallel_data(romanian_path, romani_path):\n",
        "    with open(romanian_path, 'r', encoding='utf-8') as ro_file, \\\n",
        "         open(romani_path, 'r', encoding='utf-8') as roma_file:\n",
        "        romanian_lines = ro_file.readlines()\n",
        "        romani_lines = roma_file.readlines()\n",
        "\n",
        "    assert len(romanian_lines) == len(romani_lines), \"Mismatched number of lines!\"\n",
        "\n",
        "    data = {\"translation\": [{\"ro\": ro.strip(), \"roma\": roma.strip()}\n",
        "                              for ro, roma in zip(romanian_lines, romani_lines)]}\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "\n",
        "# Tokenizing the whole dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [example[\"ro\"] for example in examples[\"translation\"]]\n",
        "    targets = [example[\"roma\"] for example in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target = targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "huggingface_dataset = load_parallel_data('romanian.txt', 'romani.txt')\n",
        "huggingface_dataset_shuffled = huggingface_dataset.shuffle(seed = 42)\n",
        "train_test_split = huggingface_dataset_shuffled.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part deals with data augmentation: a first augmentation technique was back translation. The Romanian sentences were translated to English and then the English equivalent was translated back into Romanian, while the Romani samples were kept the same.\n",
        "The second augmentation technique was sentence insertion (beginning and end). Precisely: we introduced a token at the begining of each romanian and romani sentence. The third augmentation technique was to insert a char in one of the words for each romanian and romani sample generating typos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataAugmenter:\n",
        "  def __init__(self, train_set : Dataset):\n",
        "    self.romani_data_path = 'romani.txt'\n",
        "    self.romanian_data_path = 'romanian.txt'\n",
        "    self.train_set = train_set\n",
        "\n",
        "\n",
        "  def back_translation(self):\n",
        "    # Translating the romanian sentence in english and then back into romanian to provide some variety in the dataset\n",
        "\n",
        "    augmented_sentences_list = []\n",
        "    augmented_ratio = int(0.05 * len(self.train_set))\n",
        "\n",
        "    shuffled_train_set = self.train_set.shuffle(seed = 42).select(range(augmented_ratio))\n",
        "    for sample in shuffled_train_set:\n",
        "      ro_sample, roma_sample = sample['translation']['ro'], sample['translation']['roma']\n",
        "\n",
        "      english_equivalent = GoogleTranslator(source='ro', target='en').translate(ro_sample)\n",
        "      back_translated = GoogleTranslator(source='en', target='ro').translate(english_equivalent)\n",
        "\n",
        "      augmented_sentence = {\n",
        "                \"translation\": {\n",
        "                    \"ro\": back_translated.strip(),\n",
        "                    \"roma\": roma_sample.strip(),\n",
        "                }\n",
        "            }\n",
        "      augmented_sentences_list.append(augmented_sentence)\n",
        "\n",
        "    return Dataset.from_list(augmented_sentences_list)\n",
        "  \n",
        "  def random_char_insertion(self):\n",
        "    # Replacing 2 characters in each romanian and romani sample, simulating a typo\n",
        "\n",
        "    augmented_sentences_list = []\n",
        "    augmented_ratio = int(0.05 * len(self.train_set))\n",
        "    shuffled_train_set = self.train_set.shuffle(seed = 42).select(range(augmented_ratio))\n",
        "\n",
        "    for sample in shuffled_train_set:\n",
        "      ro_sample, roma_sample = sample['translation']['ro'], sample['translation']['roma']\n",
        "\n",
        "      aug = nac.RandomCharAug(action = 'insert', aug_word_max = 1, aug_char_max = 2)\n",
        "      augmented_data_romanian, augmented_data_romani = aug.augment(ro_sample), aug.augment(roma_sample)\n",
        "      \n",
        "      new_sentence_romanian = \" \".join(augmented_data_romanian)\n",
        "      new_sentence_romani = \" \".join(augmented_data_romani)\n",
        "\n",
        "      augmented_sentence = {\n",
        "                \"translation\": {\n",
        "                    \"ro\": new_sentence_romanian.strip(),\n",
        "                    \"roma\": new_sentence_romani.strip(),\n",
        "                }\n",
        "            }\n",
        "      augmented_sentences_list.append(augmented_sentence)\n",
        "\n",
        "    return Dataset.from_list(augmented_sentences_list)\n",
        "      \n",
        "\n",
        "  def sentence_insertion(self):\n",
        "    # Inserting a token at the begining of the sentence for each romanian and romani sample\n",
        "\n",
        "    begining_token_romani = 'Po del chavo - '\n",
        "    begining_token_romanian = 'Începutul propoziției - '\n",
        "    augmented_sentences_list = []\n",
        "    augmented_ratio = int(0.05 * len(self.train_set))\n",
        "\n",
        "    shuffled_train_set = self.train_set.shuffle(seed = 42).select(range(augmented_ratio))\n",
        "\n",
        "    for sample in shuffled_train_set:\n",
        "      ro_sample, roma_sample = sample['translation']['ro'], sample['translation']['roma']\n",
        "\n",
        "      new_sentence_romanian = begining_token_romanian + ro_sample\n",
        "      new_sentence_romani = begining_token_romani + roma_sample\n",
        "\n",
        "      augmented_sentence = {\n",
        "                \"translation\": {\n",
        "                    \"ro\": new_sentence_romanian.strip(),\n",
        "                    \"roma\": new_sentence_romani.strip(),\n",
        "                }\n",
        "            }\n",
        "      augmented_sentences_list.append(augmented_sentence)\n",
        "\n",
        "    return Dataset.from_list(augmented_sentences_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if USE_AUGMENTATION:\n",
        "  augmenter = DataAugmenter(train_dataset)\n",
        "  back_translated_ds, random_char_insertion_ds, sentence_insertion_augmented_ds = augmenter.back_translation(), augmenter.random_char_insertion(), augmenter.sentence_insertion()\n",
        "  augmented_training_dataset = concatenate_datasets([train_dataset, back_translated_ds, random_char_insertion_ds, sentence_insertion_augmented_ds])\n",
        "  augmented_training_dataset = augmented_training_dataset.shuffle(seed = 42)\n",
        "  \n",
        "  tokenized_train_dataset = augmented_training_dataset.map(preprocess_function, batched=True)\n",
        "  tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "else:\n",
        "  tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "  tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load('sacrebleu')\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    \n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",             # Output directory\n",
        "    evaluation_strategy=\"epoch\",       # Evaluate every epoch\n",
        "    learning_rate=5e-5,                  # Learning rate\n",
        "    per_device_train_batch_size=16,      # Batch size for training\n",
        "    per_device_eval_batch_size=16,       # Batch size for evaluation\n",
        "    num_train_epochs=10,                  # Number of epochs\n",
        "    save_steps=500,                      # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,                  # Keep only the last 2 checkpoints\n",
        "    predict_with_generate=True,          # Use generate for evaluation\n",
        "    logging_dir=\"./logs\",              # Log directory\n",
        "    logging_steps=10,\n",
        "     # Log every 10 steps\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_to_train,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
