{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "IC_5DssHzYuR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "import sentencepiece as spm\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import MarianTokenizer, MarianMTModel, MarianConfig\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "import nlpaug.augmenter.char as nac\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_yOzudXamtm"
      },
      "source": [
        "Scraping the data from different websites/dictionaries/romani courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "hjDBYQ-cOlL1"
      },
      "outputs": [],
      "source": [
        "class Scraper:\n",
        "  # This scraper will help us gather romani texts from certain websites\n",
        "\n",
        "  def __init__(self):\n",
        "    self.romani_data_output_path = 'romani.txt'\n",
        "    self.romanian_data_output_path = 'romanian.txt'\n",
        "\n",
        "  def glosbe_romani_scraper(self, url):\n",
        "    # This site provides samples in the carpathian romani dialect\n",
        "    # Note: Some samples were added manually due to some problems with scraping\n",
        "\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    romanian_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pr-1'})\n",
        "    carpathian_romani_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pl-1'})\n",
        "\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for romanian_phrase, romani_phrase in zip(romanian_phrases, carpathian_romani_phrases):\n",
        "      ro_phrase_text, roma_phrase_text = romanian_phrase.get_text().strip(), romani_phrase.get_text().strip()\n",
        "\n",
        "      if ro_phrase_text and roma_phrase_text:\n",
        "        file_writer_romani.write(roma_phrase_text), file_writer_romani.write('\\n')\n",
        "        file_writer_romanian.write(ro_phrase_text), file_writer_romanian.write('\\n')\n",
        "\n",
        "  def get_phrases_from_dictionary_course(self, path_dict):\n",
        "    # This method gets some samples from a romanian - kalderash romani dictionary and a romani course that we found online\n",
        "    \n",
        "    dict_file = open(path_dict, 'r')\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "    for sample in dict_file:\n",
        "      phrases = sample.split(':')\n",
        "      romani_phrase, romanian_phrase = phrases[0], phrases[1].strip()\n",
        "      \n",
        "      file_writer_romani.write(romani_phrase), file_writer_romani.write('\\n')\n",
        "      file_writer_romanian.write(romanian_phrase), file_writer_romanian.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next step was to create a tokenizer. We decided to use a MarianTokenizer from HelsinkiNLP as it has support for romance languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a tokenizer and a vocabulary\n",
        "# Here you can use your custom tokenizer if you don't want to use the already existent one\n",
        "def create_tokenizer(corpus_path, tokenizer_model_prefix, vocab_size):\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input = corpus_path,\n",
        "      model_prefix = tokenizer_model_prefix,\n",
        "      vocab_size = int(vocab_size),\n",
        "      pad_id=0,\n",
        "      unk_id=1,\n",
        "      bos_id=2,\n",
        "      eos_id=3\n",
        "  )\n",
        "\n",
        "def create_vocabulary(tok_prefix_source, tok_prefix_target):\n",
        "  sp_processor_source = spm.SentencePieceProcessor()\n",
        "  sp_processor_source.load(tok_prefix_source + '.model')\n",
        "\n",
        "  sp_processor_target = spm.SentencePieceProcessor()\n",
        "  sp_processor_target.load(tok_prefix_target + '.model')\n",
        "\n",
        "  vocab_source, vocab_target = {}, {}\n",
        "\n",
        "  for id in range(sp_processor_source.get_piece_size()):\n",
        "    token_source = sp_processor_source.id_to_piece(id)\n",
        "    vocab_source[token_source] = id\n",
        "\n",
        "    token_target = sp_processor_target.id_to_piece(id)\n",
        "    vocab_target[token_target] = id\n",
        "  \n",
        "  return vocab_source, vocab_target\n",
        "\n",
        "def combine_vocabularies(vocab_source, vocab_target):\n",
        "  combined_vocab = {}\n",
        "  output_file = 'vocab.json'\n",
        "\n",
        "  for idx, (token, _) in enumerate(vocab_target.items()):\n",
        "    combined_vocab[token] = int(idx)\n",
        "  \n",
        "  idx_start_source_vocab, counter_non_duplicates = len(combined_vocab), 0 # keeping count of non duplicate items in the dictionary and also maintaining their index\n",
        "\n",
        "  for idx, (token, _) in enumerate(vocab_source.items()):\n",
        "    if token not in combined_vocab:\n",
        "      combined_vocab[token] = idx_start_source_vocab + counter_non_duplicates\n",
        "      \n",
        "      counter_non_duplicates += 1\n",
        "\n",
        "  with open(output_file, 'w') as vocab_file:\n",
        "    json.dump(combined_vocab, vocab_file, indent=4)\n",
        "\n",
        "vocab_size = '' # insert the vocab size here: use a greater size if you have more data  \n",
        "create_tokenizer('romani.txt', 'target', vocab_size)\n",
        "create_tokenizer('romanian.txt', 'source', vocab_size)\n",
        "\n",
        "vocab_source, vocab_target = create_vocabulary('source', 'target')\n",
        "combine_vocabularies(vocab_source, vocab_target)\n",
        "\n",
        "tokenizer = MarianTokenizer(\n",
        "    source_spm=\"source.model\",\n",
        "    target_spm=\"target.model\",\n",
        "    vocab=\"vocab.json\" \n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained('romanian_romani_tokenizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtaining the train, test sets and tokenizing them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_PRETRAINED = False\n",
        "USE_AUGMENTATION = True\n",
        "\n",
        "# Loading the data\n",
        "def load_parallel_data(romanian_path, romani_path):\n",
        "    with open(romanian_path, 'r', encoding='utf-8') as ro_file, \\\n",
        "         open(romani_path, 'r', encoding='utf-8') as roma_file:\n",
        "        romanian_lines = ro_file.readlines()\n",
        "        romani_lines = roma_file.readlines()\n",
        "\n",
        "    assert len(romanian_lines) == len(romani_lines), \"Mismatched number of lines!\"\n",
        "\n",
        "    data = {\"translation\": [{\"ro\": ro.strip(), \"roma\": roma.strip()}\n",
        "                              for ro, roma in zip(romanian_lines, romani_lines)]}\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "\n",
        "# Tokenizing the whole dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = [example[\"ro\"] for example in examples[\"translation\"]]\n",
        "    targets = [example[\"roma\"] for example in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target = targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained('romanian_romani_tokenizer')\n",
        "huggingface_dataset = load_parallel_data('romanian.txt', 'romani.txt')\n",
        "huggingface_dataset_shuffled = huggingface_dataset.shuffle(seed = 42)\n",
        "train_test_split = huggingface_dataset_shuffled.train_test_split(test_size=0.3)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part deals with data augmentation: a first augmentation technique was back translation. The Romanian sentences were translated to English and then the English equivalent was translated back into Romanian, while the Romani samples were kept the same.\n",
        "The second augmentation technique was sentence insertion (beginning and end). Precisely: we introduced a token at the begining of each romanian and romani sentence. The third augmentation technique was to insert a char in one of the words for each romanian and romani sample generating typos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataAugmenter:\n",
        "  def __init__(self, train_set : Dataset):\n",
        "    self.romani_data_path = 'romani.txt'\n",
        "    self.romanian_data_path = 'romanian.txt'\n",
        "    self.train_set = train_set\n",
        "\n",
        "\n",
        "  def back_translation(self):\n",
        "    # Translating the romanian sentence in english and then back into romanian to provide some variety in the dataset\n",
        "\n",
        "    augmented_sentences_list = []\n",
        "    augmented_ratio = int(0.05 * len(self.train_set))\n",
        "\n",
        "    shuffled_train_set = self.train_set.shuffle(seed = 42).select(range(augmented_ratio))\n",
        "    for sample in shuffled_train_set:\n",
        "      ro_sample, roma_sample = sample['translation']['ro'], sample['translation']['roma']\n",
        "\n",
        "      english_equivalent = GoogleTranslator(source='ro', target='en').translate(ro_sample)\n",
        "      back_translated = GoogleTranslator(source='en', target='ro').translate(english_equivalent)\n",
        "\n",
        "      augmented_sentence = {\n",
        "                \"translation\": {\n",
        "                    \"ro\": back_translated.strip(),\n",
        "                    \"roma\": roma_sample.strip(),\n",
        "                }\n",
        "            }\n",
        "      augmented_sentences_list.append(augmented_sentence)\n",
        "\n",
        "    return Dataset.from_list(augmented_sentences_list)\n",
        "  \n",
        "  def random_char_insertion(self):\n",
        "    # Replacing 2 characters in each romanian and romani sample, simulating a typo\n",
        "\n",
        "    augmented_sentences_list = []\n",
        "    augmented_ratio = int(0.05 * len(self.train_set))\n",
        "    shuffled_train_set = self.train_set.shuffle(seed = 42).select(range(augmented_ratio))\n",
        "\n",
        "    for sample in shuffled_train_set:\n",
        "      ro_sample, roma_sample = sample['translation']['ro'], sample['translation']['roma']\n",
        "\n",
        "      aug = nac.RandomCharAug(action = 'insert', aug_word_max = 1, aug_char_max = 2)\n",
        "      augmented_data_romanian, augmented_data_romani = aug.augment(ro_sample), aug.augment(roma_sample)\n",
        "      \n",
        "      new_sentence_romanian = \" \".join(augmented_data_romanian)\n",
        "      new_sentence_romani = \" \".join(augmented_data_romani)\n",
        "\n",
        "      augmented_sentence = {\n",
        "                \"translation\": {\n",
        "                    \"ro\": new_sentence_romanian.strip(),\n",
        "                    \"roma\": new_sentence_romani.strip(),\n",
        "                }\n",
        "            }\n",
        "      augmented_sentences_list.append(augmented_sentence)\n",
        "\n",
        "    return Dataset.from_list(augmented_sentences_list)\n",
        "      \n",
        "\n",
        "  def sentence_insertion(self):\n",
        "    # Inserting a token at the begining of the sentence for each romanian and romani sample\n",
        "\n",
        "    begining_token_romani = 'Po del chavo - '\n",
        "    begining_token_romanian = 'Începutul propoziției - '\n",
        "    augmented_sentences_list = []\n",
        "    augmented_ratio = int(0.05 * len(self.train_set))\n",
        "\n",
        "    shuffled_train_set = self.train_set.shuffle(seed = 42).select(range(augmented_ratio))\n",
        "\n",
        "    for sample in shuffled_train_set:\n",
        "      ro_sample, roma_sample = sample['translation']['ro'], sample['translation']['roma']\n",
        "\n",
        "      new_sentence_romanian = begining_token_romanian + ro_sample\n",
        "      new_sentence_romani = begining_token_romani + roma_sample\n",
        "\n",
        "      augmented_sentence = {\n",
        "                \"translation\": {\n",
        "                    \"ro\": new_sentence_romanian.strip(),\n",
        "                    \"roma\": new_sentence_romani.strip(),\n",
        "                }\n",
        "            }\n",
        "      augmented_sentences_list.append(augmented_sentence)\n",
        "\n",
        "    return Dataset.from_list(augmented_sentences_list)\n",
        "\n",
        "\n",
        "augmenter = DataAugmenter(train_dataset)\n",
        "back_translated_ds, random_char_insertion_ds, sentence_insertion_augmented_ds = augmenter.back_translation(), augmenter.random_char_insertion(), augmenter.sentence_insertion()\n",
        "augmented_training_dataset = concatenate_datasets([train_dataset, back_translated_ds, random_char_insertion_ds, sentence_insertion_augmented_ds])\n",
        "augmented_training_dataset = augmented_training_dataset.shuffle(seed = 42)\n",
        "\n",
        "if USE_AUGMENTATION:\n",
        "  tokenized_train_dataset = augmented_training_dataset.map(preprocess_function, batched=True)\n",
        "  tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "else:\n",
        "  tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "  tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_PRETRAINED:\n",
        "  model_to_train = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-roa-en')\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_to_train) \n",
        "\n",
        "else:\n",
        "  trained_model_config = MarianConfig()\n",
        "  model_to_train = MarianMTModel(config = trained_model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = evaluate.load('sacrebleu')\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    \n",
        "\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",             # Output directory\n",
        "    evaluation_strategy=\"epoch\",       # Evaluate every epoch\n",
        "    learning_rate=5e-5,                  # Learning rate\n",
        "    per_device_train_batch_size=16,      # Batch size for training\n",
        "    per_device_eval_batch_size=16,       # Batch size for evaluation\n",
        "    num_train_epochs=10,                  # Number of epochs\n",
        "    save_steps=500,                      # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,                  # Keep only the last 2 checkpoints\n",
        "    predict_with_generate=True,          # Use generate for evaluation\n",
        "    logging_dir=\"./logs\",              # Log directory\n",
        "    logging_steps=10,\n",
        "     # Log every 10 steps\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_to_train,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
