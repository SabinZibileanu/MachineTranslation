{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "IC_5DssHzYuR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt_tab')\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_yOzudXamtm"
      },
      "source": [
        "Firstly, we scraped some romani phrases from different websites: wikipedia [here](https://en.wiktionary.org/wiki/Category:Romani_phrasebook) and tumblr [here](https://www.tumblr.com/aj-rromale/4411570949/a-very-random-assortment-of-romani-phrases-and?redirect_to=%2Faj-rromale%2F4411570949%2Fa-very-random-assortment-of-romani-phrases-and&source=blog_view_login_wall) and from glosbe [here](https://glosbe.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "hjDBYQ-cOlL1"
      },
      "outputs": [],
      "source": [
        "class Scraper:\n",
        "  # This scraper will help us gather romani texts from certain websites\n",
        "\n",
        "  def __init__(self):\n",
        "    self.romani_data_output_path = 'romani.txt'\n",
        "    self.romanian_data_output_path = 'romanian.txt'\n",
        "\n",
        "  def wikipedia_scraper(self, url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    forbidden_pattern_list = [r'<a\\s+href=\"/wiki/[^\"]+:[^\"]+\"', r'<a\\s+href=\"http[^\"]*\"', r'<a\\s+href=\"\\/w\\/index\\.php\\?[^\"]*\"[^>]*>.*?<\\/a>']\n",
        "    references = soup.find_all('a', href = True, attrs = {'class':'', 'span':'', 'accesskey':'', 'data-mw':'', 'dir':'', 'aria-label':''})\n",
        "    romani_phrases_list = [] # This list will help us with checking the duplicates, as this method can store multiple instances of the same phrase\n",
        "\n",
        "    file_writer = open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for ref in references:\n",
        "      if not any(re.search(pattern, str(ref)) for pattern in forbidden_pattern_list):\n",
        "        if ref.get_text() not in romani_phrases_list:\n",
        "          romani_phrases_list.append(ref.get_text())\n",
        "          file_writer.write(ref.get_text()), file_writer.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "  def tumblr_scraper(self, url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    phrases = soup.find_all('p')\n",
        "    auxiliary_phrase_list = [phrase.get_text() for phrase in phrases] # This list helps us store all the phrases from the website\n",
        "\n",
        "    file_writer = open(self.romani_data_output_path, 'a')\n",
        "    \n",
        "    # Now we will only select the romani phrases\n",
        "    for phrase in auxiliary_phrase_list[1:len(auxiliary_phrase_list) - 1]:\n",
        "      romani_phrase = phrase.split('\\n')\n",
        "      file_writer.write(romani_phrase[0]), file_writer.write('\\n')\n",
        "\n",
        "  def glosbe_romani_scraper(self, url):\n",
        "    # This site helps with different dialects of romani, and so we used balkan and carpathian as it provides phrases as examples\n",
        "    # Note: Some samples were added manually due to some problems with scraping\n",
        "\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    romanian_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pr-1'})\n",
        "    carpathian_romani_phrases = soup.find_all('div', attrs = {'class':'w-1/2 dir-aware-pl-1'})\n",
        "\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "\n",
        "    for romanian_phrase, romani_phrase in zip(romanian_phrases, carpathian_romani_phrases):\n",
        "      ro_phrase_text, roma_phrase_text = romanian_phrase.get_text().strip(), romani_phrase.get_text().strip()\n",
        "\n",
        "      if ro_phrase_text and roma_phrase_text:\n",
        "        file_writer_romani.write(roma_phrase_text), file_writer_romani.write('\\n')\n",
        "        file_writer_romanian.write(ro_phrase_text), file_writer_romanian.write('\\n')\n",
        "\n",
        "  def get_phrases_from_dictionary(self, path_dict):\n",
        "    # This method gets some samples from a romanian - kalderash romani dictionary that we found online\n",
        "    \n",
        "    dict_file = open(path_dict, 'r')\n",
        "    file_writer_romanian, file_writer_romani = open(self.romanian_data_output_path, 'a'), open(self.romani_data_output_path, 'a')\n",
        "    for sample in dict_file:\n",
        "      phrases = sample.split(':')\n",
        "      romani_phrase, romanian_phrase = phrases[0], phrases[1].strip()\n",
        "      \n",
        "      file_writer_romani.write(romani_phrase), file_writer_romani.write('\\n')\n",
        "      file_writer_romanian.write(romanian_phrase), file_writer_romanian.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part deals with data augmentation: a first augmentation technique was back translation. The Romanian sentences were translated to English and then the English equivalent was translated back into Romanian, while the Romani samples were kept the same.\n",
        "The second augmentation technique was random swap. We swapped 2 random tokens from each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "\n",
        "def back_translation(path_romanian_samples, path_romani_samples):\n",
        "  romanian_samples_file, romani_samples_file = open(path_romanian_samples, 'r'), open(path_romani_samples, 'r')\n",
        "  back_translated_romanian_samples_list, romani_samples_list = [], []\n",
        "\n",
        "  for sample_ro, sample_romani in zip(romanian_samples_file, romani_samples_file):\n",
        "    english_equivalent = GoogleTranslator(source='ro', target='en').translate(sample_ro)\n",
        "    back_translated = GoogleTranslator(source='en', target='ro').translate(english_equivalent)\n",
        "\n",
        "    back_translated_romanian_samples_list.append(back_translated), romani_samples_list.append(sample_romani.strip())\n",
        "  \n",
        "  return back_translated_romanian_samples_list, romani_samples_list\n",
        "\n",
        "\n",
        "def random_swap(path_romanian_samples, path_romani_samples):\n",
        "  \n",
        "  romanian_samples_file, romani_samples_file = open(path_romanian_samples, 'r'), open(path_romani_samples, 'r')\n",
        "  random_swapped_romanian_list, random_swapped_romani_list = [], []\n",
        "\n",
        "  for sample_ro, sample_romani in zip(romanian_samples_file, romani_samples_file):\n",
        "    tokens_romanian = word_tokenize(sample_ro)\n",
        "    tokens_romani = word_tokenize(sample_romani.strip())\n",
        "\n",
        "    if len(tokens_romanian) == len(tokens_romani) and len(tokens_romanian) > 1 and len(tokens_romani) > 1:\n",
        "      index_1, index_2 = random.randint(0, len(tokens_romanian) - 1), random.randint(0, len(tokens_romanian) - 1)\n",
        "\n",
        "      while (index_1 == index_2):\n",
        "        index_2 = random.randint(0,len(tokens_romanian) - 1)\n",
        "      \n",
        "      tokens_romanian[index_1], tokens_romanian[index_2] = tokens_romanian[index_2], tokens_romanian[index_1]\n",
        "      tokens_romani[index_1], tokens_romani[index_2] = tokens_romani[index_2], tokens_romani[index_1]\n",
        "\n",
        "      romanian_sentence, romani_sentence = \" \".join(tokens_romanian), \" \".join(tokens_romani)\n",
        "\n",
        "      random_swapped_romanian_list.append(romanian_sentence), random_swapped_romani_list.append(romani_sentence)\n",
        "    \n",
        "  return random_swapped_romanian_list, random_swapped_romani_list\n",
        "\n",
        "def insert_new_samples(path_romanian_samples, path_romani_samples, function_choice):\n",
        "  if function_choice == 'random_swap':\n",
        "    ro_new_samples, romani_new_samples = random_swap(path_romanian_samples, path_romani_samples)\n",
        "    romanian_file_writer, romani_file_writer = open(path_romanian_samples, 'a'), open(path_romani_samples, 'a')\n",
        "  \n",
        "  elif function_choice == 'back_translation':\n",
        "    ro_new_samples, romani_new_samples = back_translation(path_romanian_samples, path_romani_samples)\n",
        "    romanian_file_writer, romani_file_writer = open(path_romanian_samples, 'a'), open(path_romani_samples, 'a')\n",
        "\n",
        "  for ro_sample, romani_sample in zip(ro_new_samples, romani_new_samples):\n",
        "    \n",
        "    romanian_file_writer.write(ro_sample), romanian_file_writer.write('\\n')\n",
        "    romani_file_writer.write(romani_sample), romani_file_writer.write('\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
