# -*- coding: utf-8 -*-
"""unsloth

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/unsloth-203ca4d7-7fa3-4ab8-9186-895fc8d05ee9.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250204/auto/storage/goog4_request%26X-Goog-Date%3D20250204T220633Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8b48899ff587bbccee3060bdc814545e4125548e49e2f2d1faec3eb640235a40bda3fcbe419a7c5c808d8279f73f354dc0ec68f27feb589671b8159f6c5e1ffe6c9173f09b695823a3888fa746efbb73d4a98d0cd2e89f25e7d10ef6e28631c368ddf1f5d3c37238f3328a9763a45de70efbef9b0aae36343be3d2cc05a24d03e74607e13e8566c190f6201597a9721391f4521672a290a51a8bafc05d8edb5e4ea64312b11f659f498a4777334e9e94d429fb5b6ccc35b5641452f11834d1c765bdd06e0c1ddae3e5b9c6101042fab0af8e0c465bc0ef681d5de662c00c2fc4d70a7f446171edb731c16e0646398778a67779a6a40debac1b09597d6547d135
"""

pip install sacrebleu

pip install unsloth

pip install -U wandb -q

from datasets import Dataset, concatenate_datasets
from sacrebleu import corpus_bleu
from transformers import pipeline
from unsloth import FastLanguageModel
import torch

# Loading the data
def load_parallel_data(romanian_path, romani_path):
    with open(romanian_path, 'r', encoding='utf-8') as ro_file, \
         open(romani_path, 'r', encoding='utf-8') as roma_file:
        romanian_lines = ro_file.readlines()
        romani_lines = roma_file.readlines()

    assert len(romanian_lines) == len(romani_lines), "Mismatched number of lines!"

    data = {"translation": [{"ro": ro.strip(), "roma": roma.strip()}
                              for ro, roma in zip(romanian_lines, romani_lines)]}
    return Dataset.from_dict(data)

huggingface_dataset = load_parallel_data('/kaggle/input/romani-romanian/romanian.txt', '/kaggle/input/romani-romanian/romani.txt')
huggingface_dataset_shuffled = huggingface_dataset.shuffle(seed = 42)
train_test_split = huggingface_dataset_shuffled.train_test_split(test_size=0.2)
train_dataset = train_test_split["train"]
test_eval_split = train_test_split["test"].train_test_split(test_size = 0.5)

print(huggingface_dataset[0])
print(huggingface_dataset_shuffled[0])
print(len(huggingface_dataset))
print("="*50)
print("Length of train set: ",len(train_dataset))
print("Length of test set: ", len(test_eval_split))
print(train_dataset)
print(test_eval_split)
print(test_eval_split["train"][0])
print(test_eval_split["test"][0])

# Load the Unsloth LLM
max_seq_length = 4096
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

FastLanguageModel.for_inference(model)

# # Function to translate Romanian sentences

# def translate(text):
#     prompt = f"Tradu urmatorul text din limba Română în limba Romani (vorbită de Romii din România):\nRomână: {text}\nRomani:"
#     inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).to("cuda")
#     outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)

#     decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
#     return decoded.split("\nRomani:")[1]

# from tqdm import tqdm  # Import tqdm for progress tracking

# test_dataset = test_eval_split["test"]  # Get the test dataset

# romanian_sentences = [example["translation"]["ro"] for example in test_dataset]
# romani_ground_truth = [[example["translation"]["roma"]] for example in test_dataset]  # List of lists

# # Generate translations with progress tracking
# translated_sentences = []
# for sentence in tqdm(romanian_sentences[:20], desc="Translating", unit="sentence"):
#     translated_sentences.append(translate(sentence))

# # Compute BLEU score with progress tracking
# bleu_score = corpus_bleu(translated_sentences, romani_ground_truth).score

# print("=" * 50)
# print(f"BLEU Score: {bleu_score}")
# print("=" * 50)

# print(translated_sentences)

# import gc
# for _ in range(5):
#     gc.collect()
#     torch.cuda.empty_cache()

model = FastLanguageModel.get_peft_model(
    model,
    r = 32,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",
                      "embed_tokens",
                      "lm_head",],
    lora_alpha = 32,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = True,
    loftq_config = None,
)

translate_prompt = """Tradu urmatorul text din limba Română în limba Romani (vorbită de Romii din România):
Română: {}
Romani: {}"""

EOS_TOKEN = tokenizer.eos_token
def formatting_prompts_func(examples):
    ro_texts = [entry["ro"] for entry in examples["translation"]]  # Extract 'ro'
    roma_texts = [entry["roma"] for entry in examples["translation"]]  # Extract 'roma'

    outputs = [
        translate_prompt.format(ro_text, roma_text) + EOS_TOKEN
        for ro_text, roma_text in zip(ro_texts, roma_texts)
    ]

    return {"text": outputs}

train_mapped = train_dataset.map(formatting_prompts_func, batched = True)

import wandb
wandb.login(key="3bab896b9406c75d767c42c46006b519367ec383")

from unsloth import is_bfloat16_supported
from unsloth import UnslothTrainer, UnslothTrainingArguments

trainer = UnslothTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_mapped,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,

    args = UnslothTrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 8,
,
        warmup_ratio = 0.2,
        num_train_epochs = 4,


        learning_rate = 5e-5,
        embedding_learning_rate = 1e-5,

        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 100,
        save_strategy="no",
        optim = "paged_adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "llama3.2_3b",
        report_to = "wandb",
    ),
)

gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

model.save_pretrained('llama3.2_3b')
tokenizer.save_pretrained('llama3.2_3b')

import gc
for _ in range(5):
    gc.collect()
    torch.cuda.empty_cache()

used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

selected_sample = "Daţi mai bine milostenie din lucrurile dinlăuntru, şi atunci toate vă vor fi curate."
inference_prompt = """Tradu urmatorul text din limba Română în limba Romani (vorbită de Romii din România):
Română: {}
Romani: """

FastLanguageModel.for_inference(model)
inputs = tokenizer([inference_prompt.format(selected_sample)], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 4096, use_cache = True, do_sample=False)
tokenizer.batch_decode(outputs)

prompt_template = """Tradu urmatorul text din limba Română în limba Romani (vorbită de Romii din România):
Română: {}
Romani: """
def translate(texts):
    prompts = [prompt_template.format(text) for text in texts]
    inputs = tokenizer(prompts, return_tensors="pt", truncation=True, padding=True).to("cuda")

    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True, do_sample=False)
    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    # Extract translated sentences
    translations = [text.split("\nRomani:")[1] for text in decoded]
    return translations

from tqdm import tqdm

test_dataset = test_eval_split["test"]

romanian_sentences = [example["translation"]["ro"] for example in test_dataset]
romani_ground_truth = [[example["translation"]["roma"]] for example in test_dataset]  # List of lists

batch_size = 16
translated_sentences = []


for i in tqdm(range(0, len(romanian_sentences), batch_size), desc="Translating", unit="batch"):
    batch = romanian_sentences[i : i + batch_size]  # Get batch
    translated_sentences.extend(translate(batch))  # Translate and append

# Compute BLEU score
bleu_score = corpus_bleu(translated_sentences, romani_ground_truth).score

print("=" * 50)
print(f"BLEU Score: {bleu_score}")
print("=" * 50)

translated_sentences

